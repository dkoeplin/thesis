\chapter{Background}
\label{background}

\section{Domain-Specific Languages}


\section{Parallel Patterns}
Parallel patterns are becoming a popular programming abstraction for writing
high level applications that can also be efficiently mapped to hardware
targets such as multicore~\cite{scala,haskell,delite-tecs14},
clusters~\cite{mapreduce,zaharia10spark,spartan},
GPUs~\cite{catanzaro11copperhead,micro14lee},
and FPGAs~\cite{auerbach10lime,george14fpl}. In addition, they have been shown
to provide high productivity when implementing applications in a wide variety
of domains~\cite{ecoop13sujeeth,pldi13halide}.

\input{2-background/figs/ppl-syntax}

In this work, we will focus on the parallel patterns defined in Figure~\ref{fig:ppl-syntax}.
We refer to this set as the parallel pattern language (PPL).
We will use these patterns to describe the high level
compiler's intermediate representation (IR) and corresponding rules during analyses and optimizations.
Patterns are type-parameterized such that the output type of each pattern is a tensor, where each
element of the tensor is of some type $V$. We currently restrict $V$ to be a scalar value and
do not allow nested tensors, only multidimensional tensors.
$D$-dimensional tensor types are denoted as $V^D$, while tuples of $D$ values are
denoted $V_D$. Note that this means a value of type $\mathbb{Z}_D$ can serve as a $D$-dimensional address
for a tensor of type $V^D$.
If no superscript or subscript is specified, the type is a 0-dimensional tensor ($V^0$) or, equivalently, a tuple of one element ($V_1$).
Both are equivalent to a scalar ($V = V_1 = V^0$).

As shown in Figure~\ref{fig:ppl-syntax}, we separate PPL into two groups based on dimensionality.
Figure~\ref{fig:ppl-syntax}a. lists the multi-dimensional patterns.
These patterns have an arbitrary number of dimensions in both their domain and range,
but the dimensions of the output tensor
are restricted to a statically known function of the dimensions of the input domain.
One-dimensional patterns, shown in Figure~\ref{fig:ppl-syntax}b., can output a tensor with a dynamic, data-dependent size.
All patterns generate output values by applying a function to
every index in their domain. Each pattern then merges these values into a final
output in a pattern-specific way.

\input{2-background/figs/ppl-examples}

Figure~\ref{fig:ppl-examples} shows common examples of how users typically interact with
the patterns in PPL in various high level, domain specific languages
and how those examples are correspondingly represented in PPL. In some cases, the syntactic structure is
essentially the same except that the input domain is inferred from the shape of
the input collection. Using explicit indices in the intermediate language allows
us to model more user-facing patterns and more input access patterns with fewer internal primitives.

% \begin{figure}\centering
% \begin{lstlisting}[mathescape=true]
% points: Array2D[Float](n,d) // data to be clustered
% centroids: Array2D[Float](k,d) // current centroids

% // Assign each point to closest centroid
% minDistIndex = map(n){ i =>
%   pt1 = points.slice(i, *)
%   minDistWithIndex = fold(k)((max, -1)){ j =>
%     pt2 = centroids.slice(j, *)
%     dist = fold(d)(0){ k =>
%              acc => acc + square(pt1(k) - pt2(k))
%            }{ (a,b) => a + b }
%     acc => if (acc._1 < dist) acc else (dist, j)
%   }{ (a,b) => if (a._1 < b._1) a else b }
%   minDistWithIndex._2
% }

% // Sum of points assigned to each centroid
% sums = multiFold(n)(k,d)(zeros(1,d)){ i =>
%   ((minDistIndex(i), 0), acc => {
%     pt = points.slice(i, *)
%     map(d){ j => acc(j) + pt(j) } })
% }{ (a,b) => map(d){ j => a(j) + b(j) } }

% // Number of points assigned to each centroid
% counts = multiFold(n)(k){ i => (minDistIndex(i),
%   acc => acc + 1 )}{ (a,b) => a + b }

% // Average assigned points to compute new centroids
% newCentroids = map(k,d){ (i,j) =>
%   sums(i,j) / counts(i)
% }
% \end{lstlisting}
% \caption{$k$-means clustering using the parallel patterns in Figure~\ref{fig:ppl-syntax}.}
% \label{fig:kmeans}
% \end{figure}

\emph{Map} generates a single element per index, aggregating the results into a fixed-size output collection.
Note that the value function can close over an arbitrary number of input collections, and therefore this pattern can represent classic parallel operations like \emph{map}, \emph{zip},
and \emph{zipWithIndex}.

\emph{MultiFold} is a generalization of a \emph{fold} which reduces generated values into a specified region of a (potentially) larger accumulator using an associative combine function.
The initial value $z$ is required to be an identity element of this function, and must have the same size and shape as the final output.
The main function $f$ generates an index specifying the location within the accumulator at which to reduce the generated value. We currently require the generated values to have the same arity as the full accumulator, but they may be of any size up to the size of the accumulator. Note that a traditional \emph{fold} is the special case of MultiFold where every generated value is the full size of the accumulator.
$f$ then converts each index into a function that consumes the specified slice of the current accumulator and returns the new slice. If the pattern's implementation maintains multiple partial accumulators in parallel, the combine function $c$ reduces them into the final result.

\emph{FlatMap} generates an arbitrary number of values per index.
These values are all concatenated into a flattened output.
As the output size can only be determined dynamically, we restrict FlatMap to one-dimensional domains so that dynamically growing the output is easily defined.
Note that this primitive also easily expresses a \emph{filter}.

\emph{GroupByFold} reduces generated values into one of many buckets where the bucket is selected by generating a key along with each value, i.e. it is a fused version of a \emph{groupBy} followed by a \emph{fold} over each bucket.
The operation is similar to \emph{MultiFold} except that the key-space cannot be determined in advance and so the output size is unknown.
Therefore we also restrict this operation to one-dimensional domains.




\begin{figure}
\centering
\begin{lstlisting}[language=Scala]
//data to be clustered, size n x d
val points: Array[Array[Float]] = ...

// current centroids, size k x d
val centroids: Array[Array[Float]] = ...

// Assign each point to the closest centroid by grouping
val groupedPoints = points.groupBy { pt1 =>
  // Assign current point to the closest centroid
  val minDistWithIndex = centroids.map { pt2 =>
    pt1.zip(pt2).map { case (a,b) => square(a - b) }.sum
  }.zipWithIndex.minBy(p => p._1)
  minDistWithIndex._2
}

// Average of points assigned to each centroid
val newCentroids = groupedPoints.map { case (k,v) =>
  v.reduce { (a,b) =>
    a.zip(b).map { case (x,y) => x + y }
  }.map { e => e / v.length }
}.toArray
\end{lstlisting}
\caption{K-Means clustering implemented using Scala collections. In Scala, \textunderscore\texttt{1} and \textunderscore\texttt{2} refer to the first and second value contained within a tuple.}
\label{fig:kmeans}
\end{figure}

\begin{figure}\centering
\begin{lstlisting}
points: Array2D[Float](n,d) // data to be clustered
centroids: Array2D[Float](k,d) // current centroids

// Sum and number of points assigned to each centroid
(sums,counts) = multiFold(n)((k,d),k)(zeros((k,d),k)){ i =>
  pt1 = points.slice(i, *)
  // Assign current point to the closest centroid
  minDistWithIndex = fold(k)((max, -1)){ j =>
    pt2 = centroids.slice(j, *)
    dist = fold(d)(0){ p =>
      acc => acc + square(pt1(p) - pt2(p))
    }{ (a,b) => a + b }
    acc => if (acc._1 < dist) acc else (dist, j)
  }{ (a,b) => if (a._1 < b._1) a else b }

  minDistIndex = minDistWithIndex._2
  sumFunc = ((minDistIndex, 0), acc => {
    pt = points.slice(i, *)
    map(d){ j => acc(j) + pt(j) }
  })
  countFunc = (minDistIndex, acc => acc + 1)

  (sumFunc, countFunc)
}{ (a,b) => {
  pt = map(k,d){ (i,j) => a._1(i,j) + b._1(i,j) }
  count = map(k){ i => a._2(i) + b._2(i) }
  (pt, count)
} }

// Average assigned points to compute new centroids
newCentroids = map(k,d){ (i,j) =>
  sums(i,j) / counts(i)
}
\end{lstlisting}
\caption{K-Means clustering represented using the parallel patterns in Figure~\ref{fig:ppl-syntax} after fusion and code motion.}
\label{fig:kmeans-fused}
\end{figure}

\paragraph{Example}
Now that we have defined the operations, we will use them to implement k-means clustering as an example application.
For reference, first consider $k$-means implemented using the standard Scala collections operations, as shown in Figure~\ref{fig:kmeans}.
We will use this application as a running example throughout the remainder of this paper, as it exemplifies many of the advantages of using parallel patterns as an abstraction for generating efficient hardware.
$k$-means consumes a set of $n$ sample points of dimensionality $d$ and attempts to cluster those points by finding the $k$ best cluster centroids for the samples.
This is achieved by iteratively refining the centroid values.
(We show only one iteration in Figure~\ref{fig:kmeans} for simplicity.)
First, every sample point is assigned to the closest current centroid by computing the distance between every sample and every centroid.
Then new centroid values are computed by averaging all the samples assigned to each centroid.
This process repeats until the centroid values stop changing.
Previous work~\cite{rompf12optimizing,brown16clusters,chambers10flumejava} has shown how to stage a DSL application like $k$-means, lowering it into a parallel pattern IR similar to ours, as well as how to perform multiple high-level optimizations automatically on the IR.
One of the most important of these optimizations is fusing patterns together, both vertically (to decrease the reuse distance between producer-consumer relationships) and horizontally (to eliminate redundant traversals over the same domain).
Figure~\ref{fig:kmeans-fused} shows the structure of $k$-means after it has been lowered into PPL and fusion rules have been applied.
We have also converted the nested arrays in the Scala example to our multidimensional arrays.
This translation requires the insertion of \emph{slice} operations in certain locations, which produce a view of a subset of the underlying data.
In our implementation, we use the Delite compiler framework \cite{delite-tecs14} to stage applications. For the remainder of this paper, we will assume a high-level translation from user code to PPL exists and always start from the parallel pattern representation.
