\chapter{Background}
\label{background}

\section{Domain-Specific Languages}
Domain-specific languages (DSLs) are, in general, programming languages which have specialized
constructs limited to a specific type of computation. A typical goal of
most domain-specific languages is to improve productivity in a particular domain
by increasing abstraction at the cost of generality. Unlike general-purpose languages like C and Java,
the number of operations and types of data structures in a domain-specific language
are generally limited. The benefit of this is that the language's compiler is
generally aware of each of these operations and able to specialize and optimize them.
Some domain-specific languages are standalone, including a full compiler with standard language
lexing, parsing, and type checking, others are embedded in a host general purpose language,
levaraging the existing language's compiler infrastructure while also
adding additional functionality. In this case, the semantics of the embedded language
define how code written in the host language and DSL interact.

For example, SQL~\cite{sql} is a standalone, declarative
domain-specific language originally created in 1986 for describing database operations with high level constructs
for defining operations on tables of data, but little support for other types of operations.
Compilers for SQL include query-planning optimizations which optimize the order of
operations based on domain-specific knowledge about each operator.
Similarly, Halide~\cite{pldi13halide} is an imperative language embedded in C++ specialized for image processing
and operations on arrays. Halide's syntax and compiler are specialized for linear and stenciled access patterns,
as these are extremely common in operations on images.

With the recent explosion of academic and industry interest in the machine learning (ML) domain,
a variety of ML-specific languages have been developed.
OptiML~\cite{optiml} is a mixed (imperative and functional) paradigm DSL embedded in Scala
which specializes in data-parallel operations on vectors and matrices.
Its operator abstractions allow it to automatically parallelize ML applications.
More recently, TensorFlow ~\cite{tensorflow} and PyTorch ~\cite{pytorch} are
embedded DSLs in Python for defining machine learning computation graphs based on
a limited set of operations on multi-dimensional arrays (``tensors'').

In this work, we will focus specifically on domain-specific languages in the domains under
data analytics, with a focus on image processing and machine learning.


\section{Parallel Patterns}

Parallel patterns like \emph{Map}, \emph{Reduce}, and \emph{GroupBy} are abstractions
borrowed from the functional programming paradigm
to capture information like read and write access patterns and data parallelism.
Due to their high level of abstraction, parallel patterns are becoming
popular for direct use in high level DSLs for improving productivity.
More pertinently, they have also been shown to be useful as a common abstraction in compiler
intermediate representations (IRs) across a wide variety of domains~\cite{ecoop13sujeeth,pldi13halide}.
The combination of their high level of abstraction and large amount of semantic information
means that can also be efficiently mapped to a variety of hardware
targets, including multicore CPUs~\cite{scala,haskell,delite-tecs14},
compute clusters~\cite{mapreduce,zaharia10spark,spartan},
GPUs~\cite{catanzaro11copperhead,micro14lee},
and FPGAs~\cite{auerbach10lime,george14fpl}.

\input{2-background/figs/ppl-syntax}

In this work, we will focus on the parallel patterns defined in Figure~\ref{fig:ppl-syntax}.
We refer to this set as the parallel pattern language (PPL).
We will use these patterns to describe the high level
compiler's intermediate representation (IR) and corresponding rules during analyses and optimizations.
Patterns are type-parameterized such that the output type of each pattern is a tensor, where each
element of the tensor is of some type $V$. We currently restrict $V$ to be a scalar value and
do not allow nested tensors, only multidimensional tensors.
$D$-dimensional tensor types are denoted as $V^D$, while tuples of $D$ values are
denoted $V_D$. Note that this means a value of type $\mathbb{Z}_D$ can serve as a $D$-dimensional address
for a tensor of type $V^D$.
If no superscript or subscript is specified, the type is a 0-dimensional tensor ($V^0$) or, equivalently, a tuple of one element ($V_1$).
Both are equivalent to a scalar ($V = V_1 = V^0$).

As shown in Figure~\ref{fig:ppl-syntax}, we separate PPL into two groups based on dimensionality.
Figure~\ref{fig:ppl-syntax}a. lists the multi-dimensional patterns.
These patterns have an arbitrary number of dimensions in both their domain and range,
but the dimensions of the output tensor
are restricted to a statically known function of the dimensions of the input domain.
One-dimensional patterns, shown in Figure~\ref{fig:ppl-syntax}b., can output a tensor with a dynamic, data-dependent size.
All patterns generate output values by applying a function to
every index in their domain. Each pattern then merges these values into a final
output in a pattern-specific way.

\input{2-background/figs/ppl-examples}

Figure~\ref{fig:ppl-examples} shows common examples of how users typically interact with
the patterns in PPL in various high level, domain specific languages
and how those examples are correspondingly represented in PPL. In some cases, the syntactic structure is
essentially the same except that the input domain is inferred from the shape of
the input collection. Using explicit indices in the intermediate language allows
us to model more user-facing patterns and more input access patterns with fewer internal primitives.

% \begin{figure}\centering
% \begin{lstlisting}[mathescape=true]
% points: Array2D[Float](n,d) // data to be clustered
% centroids: Array2D[Float](k,d) // current centroids

% // Assign each point to closest centroid
% minDistIndex = map(n){ i =>
%   pt1 = points.slice(i, *)
%   minDistWithIndex = fold(k)((max, -1)){ j =>
%     pt2 = centroids.slice(j, *)
%     dist = fold(d)(0){ k =>
%              acc => acc + square(pt1(k) - pt2(k))
%            }{ (a,b) => a + b }
%     acc => if (acc._1 < dist) acc else (dist, j)
%   }{ (a,b) => if (a._1 < b._1) a else b }
%   minDistWithIndex._2
% }

% // Sum of points assigned to each centroid
% sums = multiFold(n)(k,d)(zeros(1,d)){ i =>
%   ((minDistIndex(i), 0), acc => {
%     pt = points.slice(i, *)
%     map(d){ j => acc(j) + pt(j) } })
% }{ (a,b) => map(d){ j => a(j) + b(j) } }

% // Number of points assigned to each centroid
% counts = multiFold(n)(k){ i => (minDistIndex(i),
%   acc => acc + 1 )}{ (a,b) => a + b }

% // Average assigned points to compute new centroids
% newCentroids = map(k,d){ (i,j) =>
%   sums(i,j) / counts(i)
% }
% \end{lstlisting}
% \caption{$k$-means clustering using the parallel patterns in Figure~\ref{fig:ppl-syntax}.}
% \label{fig:kmeans}
% \end{figure}

\emph{Map} generates a single element per index, aggregating the results into a fixed-size output collection.
Note that the value function can close over an arbitrary number of input collections, and therefore this pattern can represent classic parallel operations like \emph{map}, \emph{zip},
and \emph{zipWithIndex}.

\emph{MultiFold} is a generalization of a \emph{fold} which reduces generated values into a specified region of a (potentially) larger accumulator using an associative combine function.
The initial value $z$ is required to be an identity element of this function, and must have the same size and shape as the final output.
The main function $f$ generates an index specifying the location within the accumulator at which to reduce the generated value. We currently require the generated values to have the same arity as the full accumulator, but they may be of any size up to the size of the accumulator. Note that a traditional \emph{fold} is the special case of MultiFold where every generated value is the full size of the accumulator.
$f$ then converts each index into a function that consumes the specified slice of the current accumulator and returns the new slice. If the pattern's implementation maintains multiple partial accumulators in parallel, the combine function $c$ reduces them into the final result.

\emph{FlatMap} generates an arbitrary number of values per index.
These values are all concatenated into a flattened output.
As the output size can only be determined dynamically, we restrict FlatMap to one-dimensional domains so that dynamically growing the output is easily defined.
Note that this primitive also easily expresses a \emph{filter}.

\emph{GroupByFold} reduces generated values into one of many buckets where the bucket is selected by generating a key along with each value, i.e. it is a fused version of a \emph{groupBy} followed by a \emph{fold} over each bucket.
The operation is similar to \emph{MultiFold} except that the key-space cannot be determined in advance and so the output size is unknown.
Therefore we also restrict this operation to one-dimensional domains.




\begin{figure}
\centering
\begin{lstlisting}[language=Scala]
//data to be clustered, size n x d
val points: Array[Array[Float]] = ...

// current centroids, size k x d
val centroids: Array[Array[Float]] = ...

// Assign each point to the closest centroid by grouping
val groupedPoints = points.groupBy { pt1 =>
  // Assign current point to the closest centroid
  val minDistWithIndex = centroids.map { pt2 =>
    pt1.zip(pt2).map { case (a,b) => square(a - b) }.sum
  }.zipWithIndex.minBy(p => p._1)
  minDistWithIndex._2
}

// Average of points assigned to each centroid
val newCentroids = groupedPoints.map { case (k,v) =>
  v.reduce { (a,b) =>
    a.zip(b).map { case (x,y) => x + y }
  }.map { e => e / v.length }
}.toArray
\end{lstlisting}
\caption{K-Means clustering implemented using Scala collections. In Scala, \textunderscore\texttt{1} and \textunderscore\texttt{2} refer to the first and second value contained within a tuple.}
\label{fig:kmeans}
\end{figure}

\begin{figure}\centering
\begin{lstlisting}
points: Array2D[Float](n,d) // data to be clustered
centroids: Array2D[Float](k,d) // current centroids

// Sum and number of points assigned to each centroid
(sums,counts) = multiFold(n)((k,d),k)(zeros((k,d),k)){ i =>
  pt1 = points.slice(i, *)
  // Assign current point to the closest centroid
  minDistWithIndex = fold(k)((max, -1)){ j =>
    pt2 = centroids.slice(j, *)
    dist = fold(d)(0){ p =>
      acc => acc + square(pt1(p) - pt2(p))
    }{ (a,b) => a + b }
    acc => if (acc._1 < dist) acc else (dist, j)
  }{ (a,b) => if (a._1 < b._1) a else b }

  minDistIndex = minDistWithIndex._2
  sumFunc = ((minDistIndex, 0), acc => {
    pt = points.slice(i, *)
    map(d){ j => acc(j) + pt(j) }
  })
  countFunc = (minDistIndex, acc => acc + 1)

  (sumFunc, countFunc)
}{ (a,b) => {
  pt = map(k,d){ (i,j) => a._1(i,j) + b._1(i,j) }
  count = map(k){ i => a._2(i) + b._2(i) }
  (pt, count)
} }

// Average assigned points to compute new centroids
newCentroids = map(k,d){ (i,j) =>
  sums(i,j) / counts(i)
}
\end{lstlisting}
\caption{K-Means clustering represented using the parallel patterns in Figure~\ref{fig:ppl-syntax} after fusion and code motion.}
\label{fig:kmeans-fused}
\end{figure}

\paragraph{Example}
Now that we have defined the operations, we will use them to implement k-means clustering as an example application.
For reference, first consider $k$-means implemented using the standard Scala collections operations, as shown in Figure~\ref{fig:kmeans}.
We will use this application as a running example throughout the remainder of this paper, as it exemplifies many of the advantages of using parallel patterns as an abstraction for generating efficient hardware.
$k$-means consumes a set of $n$ sample points of dimensionality $d$ and attempts to cluster those points by finding the $k$ best cluster centroids for the samples.
This is achieved by iteratively refining the centroid values.
(We show only one iteration in Figure~\ref{fig:kmeans} for simplicity.)
First, every sample point is assigned to the closest current centroid by computing the distance between every sample and every centroid.
Then new centroid values are computed by averaging all the samples assigned to each centroid.
This process repeats until the centroid values stop changing.
Previous work~\cite{rompf12optimizing,brown16clusters,chambers10flumejava} has shown how to stage a DSL application like $k$-means, lowering it into a parallel pattern IR similar to ours, as well as how to perform multiple high-level optimizations automatically on the IR.
One of the most important of these optimizations is fusing patterns together, both vertically (to decrease the reuse distance between producer-consumer relationships) and horizontally (to eliminate redundant traversals over the same domain).
Figure~\ref{fig:kmeans-fused} shows the structure of $k$-means after it has been lowered into PPL and fusion rules have been applied.
We have also converted the nested arrays in the Scala example to our multidimensional arrays.
This translation requires the insertion of \emph{slice} operations in certain locations, which produce a view of a subset of the underlying data.
For the remainder of this work, we will assume a high-level translation from user DSL code to PPL exists and always start from the parallel pattern intermediate representation.
