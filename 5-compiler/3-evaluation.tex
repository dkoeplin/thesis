\section{Evaluation}
\label{spatial-evaluation}

While Spatial's primary goal is to serve as an intermediate hardware abstraction
for compiling DSLs to FPGAs, this level of abstraction also makes it competitive
in terms of programmer productivity with existing HLS tools.
In this section, we first evaluate both of our approaches to automated design tuning,
first with random search with heuristic pruning,
then using the HyperMapper design tuning approach.
Using the generated points from the random search, we then
evaluate the Spatial compiler's area and runtime modeling accuracy.
We then evaluate Spatial and frontend language by comparing programmer productivity
and the performance of generated designs to Xilinx's commercial HLS tool, SDAccel.

To conlude, we also demonstrate Spatial's generality by showing its ability
to target multiple kinds of reconfigurable hardware from the same source code.
showing both portability across FPGAs and on the recently proposed Plasticine CGRA~\cite{plasticine}.

\subsection{Design Tuning with Random Search}

\begin{table}
\centering\footnotesize
\begin{tabular}{lm{3.6cm}m{2cm}}
\toprule
{\bf Benchmark} & {\bf Description} & {\bf Dataset Size} \\ \midrule
dotproduct   & Vector dot product             & 187,200,000\\ \midrule
outerproduct & Vector outer product           & $38,400$ $38,400$ \\ \midrule
gemm         & Tiled matrix multiplication    & $1536 \times 1536$\\ \midrule
tpchq6       & TPC-H Query 6                  & N=18,720,000 \\ \midrule
blackscholes & Black-Scholes-Merton model     & N=9,995,328 \\ \midrule
gda          & Gaussian discriminant analysis & R=360,000 D=96 \\ \midrule
kmeans       & $k$-Means clustering           & \#points=960,000, k=8, dim=384 \\ \midrule
\end{tabular}
\caption{Evaluation benchmarks.}
\label{t:benchmarks}
\end{table}

\subsubsection{Benchmarks}
Table~\ref{t:benchmarks} lists the benchmarks we use in the evaluation of our
design tuning with random search along with the corresponding
input dataset sizes used. \emph{Dotproduct}, \emph{outerprod}, and \emph{gemm} are common
linear algebra kernels. \emph{Tpchq6} is a data analytics application that streams through a collection
of records and performs a reduction on records filtered by a condition. \emph{BlackScholes} is
a financial analytics application that implements Black-Scholes option pricing. \emph{Gda}
and \emph{kmeans}
are commonly used machine learning kernels used for data classification and clustering, respectively.
All benchmarks operate on single-precision floating point numbers,
except in certain cases where the benchmark requires integer or boolean values as inputs.
All benchmarks were written in Spatial by hand but are equivalent to what
could be generated automatically from higher level DSLs.

\subsubsection{Methodology}
Each generated design is synthesized and run on an Altera 28nm Stratix V FPGA
on a Max4 MAIA board at a fabric clock frequency of 150MHz. The MAIA board interfaces with an Intel CPU
via PCIe. The board has 48GB of dedicated off-chip DDR3 DRAM with a peak bandwidth of 76.8GB/s. In practice,
our maximum memory bandwidth is 37.5 GB/s, as our on-chip memory clock is limited to 400MHz.
For this board, we leverage Maxeler's runtime to manage communication and data movement
between the host CPU and the MAIA board.
Execution time is measured starting
from when the FPGA design is started (after input has been copied to FPGA DRAM) and stopped after
the design finishes execution (before output is copied to CPU DRAM). We report execution time as the
average of 20 runs to eliminate noise from design initialization time and off-chip memory latencies.
The FPGA resource utilization
numbers reported are from the post place-and-route report generated by Altera's logic synthesis tools. Logic (LUT) resources are reported in terms of Altera's adaptive logic module (ALM) resources.

\begin{figure*}[!htbp]
\centering
\includegraphics[width=0.95\textwidth]{figs/tradeoff.pdf}
\caption{Results of design space exploration. Horizontal axis shows estimated ALM, DSP, and BRAM usages. Vertical axis shows runtime in cycles, given in log scale (base 10).}
\label{fig:dse}
\end{figure*}

\subsubsection{Pareto-Optimality Analysis}
Figure~\ref{fig:dse} shows the resulting design space scatter plots for all benchmarks in
Table~\ref{t:benchmarks}. A design point is considered
invalid if its resource requirement for at least one type of resource exceeds the maximum
available amount on the target device. Pareto-optimal designs along the dimensions of execution time
and ALM utilization are highlighted for each benchmark through all three resource plots.
We now analyze each benchmark in detail.

\textbf{Dot product} (Figure~\ref{fig:dse} A,B,C) is a memory-bound benchmark. Peak execution
time is reached by balancing tile loads and computation. Inner and outer loop parallelization allows us to
quickly reach close to the input bandwidth. Runtimes of designs with pipelined loops then slowly decrease as parallelization
increases once the dominant stage becomes the dot product reduction tree. In \emph{dotproduct}, designs with pipelines consume fewer resources than those with \emph{Sequential} for the same performance. \emph{Sequential}s require larger tile sizes and more parallelism to match pipelined performance.

\textbf{Outer product} (Figure~\ref{fig:dse} D,E,F) represents both a BRAM and memory bound benchmark. For $2N$ inputs,
the total BRAM requirement is $2N + N^2$ to store the input and output tiles, meaning the BRAM requirement
increases quadratically with increases in input tile size. The highest performing designs for outer product do not use
pipelines to overlap loading and storing of tiles. This is because the overhead due to main memory contention from overlapping
tile loads and stores turns out to be higher than the cost of executing each stage sequentially.

\textbf{GEMM} (Figure~\ref{fig:dse} G,H,I) contains a lot of temporal and spatial locality. From Figure~\ref{fig:dse}(I), Pareto-optimal designs for \emph{gemm}
occupy almost all BRAM resources on the board. Intuitively, this is  because good designs for \emph{gemm}
maximize locality by retaining large, two dimensional chunks of data in on-chip memory.
%However, as seen in Figure~\ref{fig:dse}(G), \emph{gemm} is also limited by the
%number of ALMs due to the large number of floating point operations being done in parallel.

\textbf{TPC-H Q6} (Figure~\ref{fig:dse} J,K,L) exhibits behavior typical of memory intensive applications. Performance reaches a maximum threshold
with increased tile size because of overlapping memory access and compute.

\textbf{BlackScholes} (Figure~\ref{fig:dse} M,N,O) streams through multiple large arrays and performs complex floating point computations
on the input data. Points along the same vertical bar in Figure~\ref{fig:dse}(M) share the same inner loop parallelization
factor. Increasing parallelization improves performance by increasing utilization of the available off-chip memory bandwidth.
Our model suggests that increasing the inner loop parallelization would continue to scale
performance until a parallelization of 16, around which point \emph{blackscholes} would be memory bound. Because there are not enough compute resources are available to implement a parallelization factor of 16, \emph{blackscholes} is ALM bound.

\textbf{GDA} (Figure~\ref{fig:dse} P,Q,R) possesses higher degrees of spatial locality. Because of this, \emph{gda} exhibits compute-bound behavior, where execution time
decreases steadily with increased resource utilization, as seen in Figure~\ref{fig:dse}(P). The critical resource is again BRAM. This is because BRAM usage increases with parallelization due to the creation of
more banks with fewer words per bank, which can cause under-utilization of the capacity of individual BRAMs.

\textbf{K-Means} (Figure~\ref{fig:dse} S,T,U) is bound by the number of ALMs. The critical path in this application is the distance computation done comparing an input point to each centroid.
The number of floating point operations to be done to keep up with main memory bandwidth is therefore proportional to $K \times D$, where $D$ is the number of dimensions in one point.
The performance of \emph{kmeans} is therefore limited by the number of ALMs on the FPGA, as not enough are available to perform all $K \times D$ operations in parallel.
Like GDA, \emph{kmeans} is also limited by BRAMs due to under-utilization of BRAM capacity with increased banking factors.

From our experiments, we observe that capturing parallelism at multiple levels using pipelined controllers enables us to generate
efficient designs. In addition, effective management of on-chip BRAM resources is critical to good designs
as BRAM resources are the limiting factor for performance scaling in most of our benchmarks.

\subsection{Design Tuning Speed}
We compare the speed of our estimation and design space exploration with
Vivado HLS~\cite{vivadohls}, a commercial high-level synthesis tool from Xilinx.
Our evaluation uses the GDA example in Figure~\ref{fig:gda-hls} as input to the high-level synthesis tool, and a GDA design written in Spatial as input to our design space exploration tool. Design parameters
for the high-level synthesis tool are the unrolling factors. We also include a pipeline directive
toggle for each loop in the design. Design tuning speed
is measured by comparing the average estimation speed per point for 250 design points for each tool.
In our experiments, our analysis takes 5 to 29 milliseconds per design depending on the size of the application's intermediate representation. Analysis of GDA also takes 17 milliseconds per design.

\begin{table*}
\centering\footnotesize
\begin{tabular}{lcc}
\toprule
{\bf Our approach}  & {\bf Vivado HLS restricted\textsuperscript{$\dagger$}} & {\bf Vivado HLS full} \\ \midrule
0.017s / design     & 4.75s / design               & 111.06s / design      \\ \midrule
\end{tabular}

\vspace{5pt}
\textsuperscript{$\dagger$}Vivado HLS restricted design space ignores outer loop pipelining
\caption{Average estimation time per design point.}
\label{t:speeds}
\end{table*}

Table~\ref{t:speeds} shows a comparison between estimation speeds from our toolchain and Vivado HLS.
The ``restricted'' column refers to the average time spent per design over points whose outer loop ($L1$, in Figure~\ref{fig:gda-hls})
is not pipelined with a pipeline directive. The ``full'' version refers to all design points where 30 of the 250 points
have a pipeline directive to enable outer loop pipelining. We observe the following:
\begin{itemize}
  \item Our estimation tool is 279$\times$ faster than the ``restricted'' space exploration, and 6533$\times$ faster than the ``full'' space exploration.
  \item Compared to Vivado HLS, our estimation time is not sensitive to design parameter inputs. Estimation time for Vivado HLS increases
    dramatically when the outer loop is pipelined in GDA because the tool completely unrolls all inner loops
    before pipelining the outer loop. This creates a large graph that complicates scheduling. Our approach does not
    suffer from this limitation because we explicitly capture pipelines with parameterized controllers, thereby capturing outer loop pipelining more naturally.
\end{itemize}

\subsection{Design Tuning with Active Learning}
We next perform a preliminary evaluation of HyperMapper for quickly approximating Pareto frontier over two design objectives: design runtime and FPGA logic utilization (LUTs).
For this evaluation, we run HyperMapper with several seeds of initial random sample, with the number of samples $R$ ranging from 1 to 6000 designs, and run 5 active learning iterations of at most 100 samples each. For comparison, the heuristic search proposed in the DHDL work~\cite{dhdl} prunes using simple heuristics and then randomly samples up to 100,000 points. For both approaches, design tuning takes up to 1 -- 2 minutes, varying slightly by benchmark complexity.

Figure~\ref{hvi_samples} shows the hypervolume indicator (HVI) function
for the \emph{BlackScholes} benchmark as a function of the initial
number of random samples.
The HVI gives the area between the estimated Pareto frontier and the space's true Pareto curve, found from exhaustive search.
By increasing
the number of random samples to bootstrap the active learning phase,
we see two orders
of magnitude improvement in HVI. Furthermore, the overall variance goes down very
quickly as the number of random
samples increases. As a result, the autotuner is robust
to randomness and only a handful of random samples are
needed to bootstrap the active learning phase. As shown in Figure~\ref{paretos},
HyperMapper is able to reach a close approximation of
the true Pareto frontier with less than 1500 design points.

On benchmarks like GDA with sparser design spaces, HyperMapper spends much of its time evaluating areas
of the space with invalid designs which cannot fit on the FPGA.
HyperMapper's accuracy for these benchmarks is consequently lower than the heuristic approach.
Consequently, followup work is looking at expanding HyperMapper with a
valid design prediction mechanism~\cite{hypermapper2}.

\begin{figure}
\centering
%%% trim = left, bottom, right, top
\begin{subfigure}[t]{0.45\linewidth}
\includegraphics[width=\linewidth]{5-compiler/figs/hvi_5NumberSummary_median.png}
\subcaption{HyperMapper HVI versus initial random samples ($R$) five number summary.}
\label{hvi_samples}
\end{subfigure}\hspace{15pt}
~
\begin{subfigure}[t]{0.45\linewidth}
\includegraphics[width=\linewidth]{5-compiler/figs/output_pareto_blackscholes.pdf}
\subcaption{Exhaustive and HyperMapper ($R$=1000) generated Pareto curves. }
\label{paretos}
\end{subfigure}

\vspace{-10pt}
\caption{Design space tuning on \emph{BlackScholes}.}
\label{figHVI}
\end{figure}


\subsection{Modeling Accuracy}

\begin{table}
\centering\footnotesize
\begin{tabular}{lcccc}
\toprule

{\bf Benchmark} & {\bf ALMs} & {\bf DSPs} & {\bf BRAM} & {\bf Runtime} \\ \midrule
dotproduct      & 1.7\%      & 0.0\%      & 13.1\%      & 2.8\%  \\ \midrule
outerproduct    & 4.4\%      & 29.7\%     & 12.8\%      & 1.3\%  \\ \midrule
gemm            & 12.7\%     & 11.4\%     & 17.4\%      & 18.4\% \\ \midrule
tpchq6          & 2.3\%      & 0.0\%      & 5.4\%       & 3.1\%  \\ \midrule
blackscholes    & 5.3\%      & 5.3\%      & 7.0\%       & 3.4\%  \\ \midrule
gda             & 5.2\%      & 6.2\%      & 8.4\%       & 6.7\%  \\ \midrule
kmeans          & 2.0\%      & 0.0\%      & 21.9\%      & 7.0\%  \\ \midrule \midrule

Average         & 4.8\%      & 7.5\%     & 12.3\%      & 6.1\%   \\ \bottomrule
\end{tabular}
\caption{Average absolute error for resource usage and runtime.}
\label{t:errors}
\end{table}

We next evaluate the absolute accuracy of our modeling approach. We select five Pareto points generated from our design space exploration for each of our benchmarks. We then generate and synthesize hardware for each design and run it on the FPGA. We compare our area estimates to post place-and-route reports generated by Altera's toolchain. We then run the design on the FPGA and compare the estimated runtime to observed runtime. Note that runtime includes off-chip memory accesses from the FPGA to its DRAM. Table~\ref{t:errors} summarizes the errors averaged across all selected Pareto points for each benchmark.

Our area estimates have an average error of 4.8\% for ALMs, 7.5\% for DSPs, and 12.3\% for BRAMs, while our runtime estimation error averages 6.1\%.
Our highest error occurs in predicting DSPs for \emph{outerprod}, where we over-predict by 29.7\% DSP usage on average.
However, we found that errors above 10\% for DSP usage only occur for designs which use less than 2\% of the total DSPs available on the device.
As our benchmarks are limited by other resources (typically ALMs or BRAM), the relative error for DSPs is more sensitive
to low-level fluctuations and noise. We observe that our DSP estimates preserve absolute ordering of resource utilization. Hence, this error does not
affect the quality of the designs found during design space exploration.%, and improves with increased resource utilization.

Of our estimated metrics, BRAM estimates have the highest average error over all benchmarks. These errors are primarily from block RAM duplication done by the placement and routing tool. In designing our models, we found that BRAM duplication is inherently noisy, as more complex machine learning models failed to achieve better estimates than a simple linear fit. Our linear model provides a rough estimate of design complexity and routing requirements, but it does not provide a complete picture for when and how often the synthesis tool will decide to duplicate BRAMs. However, like DSPs, we find that our BRAM estimates track actual usage and preserve ordering across designs, making it usable for design space exploration and relative design comparisons.

\emph{GEMM} has the highest overall error of any benchmark. We found that this is due to low-level hardware optimizations like floating point multiply-add fusion, fusion of floating point reduction trees, and BRAM coalescing that Maxeler's compiler performs automatically and that we use heuristics to predict. Since we do not have explicit control over these optimizations, it is possible to mispredict when they will occur. The \emph{gemm} benchmark is exceptionally sensitive to these errors. However, as with the other errors, we found that this error does not detract from the model's ability to guide design space exploration as long as the possibility of this error is accounted for.


\subsection{FPGA Performance and Productivity}

\input{5-compiler/figs/eval_sdaccel}

We next evaluate the FPGA performance and productivity benefits of Spatial against SDAccel, a commercial C-based programming tool from Xilinx for creating high-performance accelerator designs. We use SDAccel in our study as it has similar performance and productivity goals as Spatial and performs several optimizations related to loop pipelining, unrolling, and memory partitioning~\cite{sdaccel}. Baseline implementations of the benchmarks in Table~\ref{t:hls_comp} have been either obtained from a public SDAccel benchmark suite from Xilinx~\cite{sdaccelBench}, or written by hand. Each baseline has then been manually tuned by using appropriate HLS pragmas~\cite{hlsPragmaRef} to pick loop pipelining, unrolling, and array banking factors, and to enable dataflow optimizations. Design points for Spatial are chosen using the DSE flow described in Section~\ref{dse}.

We measure productivity by comparing number of lines of source code used to describe the FPGA kernel, excluding host code. We measure performance by comparing runtimes and FPGA resources utilized for each benchmark on a Xilinx Ultrascale+ VU9P board with a fabric clock of 125 MHz, hosted on an Amazon EC2 F1 instance. We generate FPGA bitstreams targeting the VU9P architecture for each benchmark using both Spatial and SDAccel, and obtain resource utilization data from the post place-and-route reports. We then run and verify both designs on the FPGA and measure the execution times on the board. CPU setup code and data transfer time between CPU and FPGA is excluded from runtime measurements for both tools.

Table~\ref{t:hls_comp} shows the input dataset sizes and the full comparison between lines of source code, resource utilization, and runtime of the benchmarks implemented in SDAccel and Spatial.
In terms of productivity, language constructs in Spatial like \texttt{\small{load}} and \texttt{\small{store}} for transferring dense sparse data from DRAM reduces code bloat and increases readability.
Furthermore, by implicitly
inferring parameters such as parallelization factors and loop initiation intervals, Spatial code is largely free of annotations and pragmas.

Spatial achieves speedups over SDAccel of $1.63\times$ and $1.33\times$ respectively on \emph{BlackScholes} and \emph{TPC-H Q6}. Both benchmarks
stream data from DRAM through a deeply pipelined datapath which is amenable to FPGA acceleration. Dataflow support in SDAccel using the DATAFLOW pragma~\cite{dataflowRef} and streaming support in Spatial allows both tools to efficiently accelerate such workloads. In \emph{K-Means}, coarse-grained pipelining support allows Spatial to achieve roughly the same performance as SDAccel using $1.5\times$ fewer BRAMs.
Specialized DRAM scatter/gather support enables Spatial to achieve a $3.48\times$ speedup on \emph{PageRank}.

We see speedups of $8.48\times$, $1.37\times$, and $14.15\times$ for compute-heavy workloads \emph{GDA}, \emph{GEMM}, and \emph{SW}, respectively. The baseline for \emph{SW} is implemented by Xilinx as a systolic array, while the Spatial implementation uses nested controllers. \emph{GEMM} and \emph{GDA} contain opportunities for coarse-grained pipelining that are exploited within Spatial.
GDA, for example, contains an outer product operation, during which the data in the same buffer is repeatedly accessed and reused. While this operation can be pipelined with a preceding loop producing the array, SDAccel's DATAFLOW pragma does not support such access patterns that involve reuse. As a result, SDAccel requires larger array partitioning and loop unrolling factors to offset the performance impact, at the expense of consuming more FPGA BRAM.
In addition, nested controllers in \emph{GEMM}
can be parallelized and pipelined independently in Spatial, while SDAccel automatically unrolls all inner loops if an outer loop is parallelized. Spatial can therefore explore
design points that cannot be easily expressed in SDAccel. Finally, as the Spatial compiler performs analyses on a parameterized IR, the compiler can reason about larger parallelization factors without expanding the IR graph.
SDAccel unrolls the graph as a preprocessing step, hence creating larger graphs when unrolling and array partitioning factors are large.
This has a significant impact on the compiler's memory footprint and compilation times, making better designs difficult or impossible to find.

Spatial provides a productive platform to program FPGAs, with a 42\% reduction in lines of code compared to SDAccel averaged across all benchmarks. On the studied benchmarks, Spatial achieves a geometric mean speedup of $2.9\times$ compared to an industrial HLS tool.


\subsection{Spatial FPGA Portability}

\begin{table}
\centering
\fontsize{7}{9}\selectfont
\begin{tabular}{l d{2.1} d{2.1} d{2.1} d{2.1} d{2.1} d{2.1}}
   \bf{FPGA}      & \mc{ZC706}  & \multicolumn{4}{c}{\bf VU9P}                                       & \mc{Total}     \\
   \bf{Design}    & \mc{Tuned}  & \multicolumn{2}{c}{\bf Ported}   & \multicolumn{2}{c}{\bf Tuned}    &               \\ \toprule

                  & \mc{Time}   & \mc{Time}  & \mc{$\times$}       & \mc{Time}  & \mc{$\times$}       & \mc{$\times$} \\ \midrule
   \ml{BS}        & 89.0        & 35.6       & 2.5                 & 3.8        & 9.4                 & 23.4          \\ \midrule
   \ml{GDA}       &  8.4        & 3.4        & 2.5                 & 1.3        & 2.6                 & 6.5           \\ \midrule
   \ml{GEMM}      & 2226.5      & 1832.6     & 1.2                 & 878.5      & 2.1                 & 2.5           \\ \midrule
   \ml{KMeans}    & 358.4       & 143.4      & 2.5                 & 53.3       & 2.7                 & 6.7           \\ \midrule
   \ml{PageRank}  & 1299.5      & 1003.3     & 1.3                 & 587.4      & 1.7                 & 2.2           \\ \midrule
   \ml{SW$^\dag$} & 1.3         &  0.5       & 2.5                 & 0.5        & 1.0                 & 2.5           \\ \midrule
   \ml{TQ6}       & 69.4        & 15.0       & 4.6                 & 14.0       & 1.1                 & 5.0           \\ \bottomrule

   \multicolumn{7}{l}{\vspace{10pt}\footnotesize{ $^\dag$SW with 160 base pairs, the largest to fit on the ZC706.}}
\end{tabular}
\caption{Runtimes (ms) of tuned designs on ZC706, followed by runtimes and speedup~($\times$) of directly porting these designs to the VU9P, then runtimes and successive speedup over ported designs when tuned for the VU9P. The \emph{Total} column shows the cumulative speedup.}
\label{fig:zynq_comp}
\end{table}

Next, we demonstrate the portability of Spatial code by targeting two different FPGA architectures; (1) the Zynq ZC706 SoC board, and (2) The Virtex Ultrascale+ VU9P on the Amazon EC2 F1.
Designs on the VU9P use a single DRAM channel with a peak bandwidth of 19.2 GB/s. The ZC706 is much smaller than the VU9P in terms of FPGA resource and has a smaller DRAM bandwidth of 4.26 GB/s.
We target both the ZC706 and VU9P from the same Spatial code for all benchmarks listed in Table~\ref{t:hls_comp}. Benchmarks are tuned for each target using
target-specific models with automated DSE. Clock frequency is fixed at 125 MHz for both FPGAs.

Table~\ref{fig:zynq_comp} shows the speedups achieved on the VU9P over the ZC706. The results show that not only can the same Spatial source code be ported
to architectures with different capabilities, the application can also be automatically tuned to better take advantage of resources in each target.
Compute-bound benchmarks \emph{BlackScholes}, \emph{GDA}, \emph{GEMM}, \emph{K-Means} achieve speedups of up to $23\times$ on the VU9P over the ZC706. Porting these designs to the VU9P alone has a $1.2\times$ to $2.5\times$ due to increased main memory bandwidth, but a majority of the benefit of the larger FPGA comes from tuning the parallelization factors to use more resources.
While \emph{SW} is also compute bound, the size of the dataset was limited by the smaller FPGA. In this case, the larger capacity of the VU9P does not improve runtime, but instead allows handling of larger datasets.

Memory-bound benchmark \emph{TPC-H Q6} benefits from the higher DRAM bandwidth available on the VU9P. Porting this benchmark immediately gives a $4.6\times$ runtime improvement from the larger main memory bandwidth, while further parallelizing controllers to create more parallel address streams to DRAM helps the application make better use of this bandwidth. \emph{PageRank} is also bandwidth-bound, but the primary benefit on the VU9P comes from specializing the memory controller to maximize utilized bandwidth for sparse accesses.


% \begin{table}
% \caption{Speedup of VU9P over ZC706.}
% \label{fig:zynq_comp}

% \fontsize{8}{10}\selectfont
% \begin{tabular}{cccccc}
% \bf{GDA}     & \bf{GEMM}    & \bf{K-Means} & \bf{PageRank} & \bf{SW}      & \bf{TQ6} \\ \hline
% 2.54$\times$ & 2.53$\times$ & 1.84$\times$ & 2.21$\times$  & 1.74$\times$ & 4.97$\times$  \\ \hline
% \end{tabular}
% \end{table}

\subsection{Spatial Generality Beyond FPGAs}
\label{plasticine-evaluation}

\begin{table}
\centering
\fontsize{7}{7}\selectfont
  \begin{tabular}{m{0.5cm} d{2.1} d{2.1} d{2.1} d{2.1} d{2.1} d{2.1} r }
  \toprule
                 & \multicolumn{2}{c}{\bf Avg DRAM } & \multicolumn{3}{c}{\bf Resource }        & \mc{}     & \mc{} \\
                 & \multicolumn{2}{c}{\bf BW (\%)}   & \multicolumn{3}{c}{\bf Utilization (\%)} & \mc{Time} & \mc{$\times$} \\
   \bf{App}     & \mc{Load}    & \mc{Store} & \mc{PCU}  & \mc{PMU}  & \mc{AG}   & \mc{(ms)} & \mc{} \\ \midrule
   \ml{BS}       & 77.4        & 12.9       & \mb{\hspace{1pt}73.4} & 10.9      & 20.6      & 2.33      & 1.6   \\
   \ml{GDA}      & 24.0        & 0.2        & \mb{\hspace{1pt}95.3} & 73.4      & 38.2      & 0.13      & 9.8   \\
   \ml{GEMM}     & 20.5        & 2.1        & \mb{\hspace{1pt}96.8} & 64.1      & 11.7      & 15.98     & 55.0  \\
   \ml{KMeans}   & 8.0         & 0.4        & \mb{\hspace{1pt}89.1} & 57.8      & 17.6      & 8.39      & 6.3   \\
   \ml{TQ6}      & \mb{\hspace{2pt}97.2}   & 0.0        & 29.7      & 37.5      & \mb{70.6} & 8.60      & 1.6   \\

\bottomrule
\end{tabular}
\caption{Plasticine DRAM bandwidth, resource utilization, runtime, and speedup ($\times$) over a Xilinx
VU9P FPGA. Both Plasticine and FPGA implementations were generated from the same Spatial source code.}
\label{table:plasticine_eval}
\end{table}

Finally, we demonstrate the portability of Spatial beyond
FPGA architectures and its generality across reconfigurable architectures
by including an evaluation of the compiler to map
the Spatial IR to target the Plasticine CGRA~\cite{plasticine,plasticine2}. Plasticine is a
two-dimensional array of compute (PCUs) and memory
(PMUs) tiles with a statically configurable interconnect
and address generators (AG) at the periphery to perform
DRAM accesses. The Plasticine architecture is a significant departure
from an FPGA, with more constraints on memory banking and computation, including
fixed size, pipelined SIMD lanes. More details about the compiler extensions
required to target Plasticine can be found in the original work
on the Plasticine CGRA~\cite{plasticine}.

We simulate Plasticine with  a $16 \times 8$ array of 64 compute and 64 memory tiles, with a 1 GHz clock and a main memory with a DDR3-1600 channel with 12.8 GB/s peak bandwidth.
Table~\ref{table:plasticine_eval} shows the DRAM bandwidth, resource utilization, runtime, and speedup of the Plasticine CGRA over the VU9P for a subset of benchmarks.

Streaming, bandwidth-bound applications like \emph{TPC-H Q6} efficiently exploit about 97\% of the available DRAM bandwidth.
Compute-bound applications \emph{GDA}, \emph{GEMM}, and \emph{K-Means} use around 90\% of Plasticine's compute tiles.
Plasticine's higher on-chip bandwidth also allows these applications to better utilize the compute resources, giving these applications speedups of $9.9\times$, $55.0\times$, and $6.3\times$.
Similarly, the deep compute pipeline in \emph{BlackScholes} occupies 73.4\% of compute resources after being split across multiple tiles,
giving a speedup of $1.6\times$.
