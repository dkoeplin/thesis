\chapter{Introduction}
\label{intro}

\section{Reconfigurable Architectures}
Recent trends in technology scaling, the availability of large amounts of data, and novel algorithmic breakthroughs
have spurred accelerator architecture research. Reconfigurable architectures like field-programmable gate arrays (FPGAs)
and coarse-grain reconfigurable architectures (CGRAs)
have received renewed interest from academic researchers and industry practitioners alike,
primarily due to their potential performance and energy efficiency benefits over conventional CPUs.
FPGAs are now being used to accelerate web search
in datacenters at Microsoft and Baidu~\cite{catapult, baidu},
Amazon now offers FPGA instances as part of AWS~\cite{awsf1},
and Intel has announced products like in-package Xeon-FPGA systems~\cite{harp}
and FPGA-accelerated storage systems~\cite{nand_flash}.
Similarly, several recent research prototypes~\cite{dyser, ti, scaledeep, scnn, plasticine}
and startups~\cite{wavecomp, nervana} have explored various
kinds of CGRAs at different granularities.
Growing use of such reconfigurable architectures has made them more available to programmers now than ever before.

Reconfigurable devices are able to accelerate applications, in part, by exploiting multiple
levels of nested parallelism and data locality with custom data pipelines and memory hierarchies.
Unfortunately, the same features that make reconfigurable architectures efficient
also make them much more complex to program. An accelerator design must account for the timing between pipelined signals and
the physically limited compute and memory resources available on the target device.
It must also manage partitioning of data between local scratchpads and off-chip memory to achieve good data locality.
The combination of these complexities leads to intractable accelerator design spaces, even for relatively small applications~\cite{cascaval}.

These challenges have caused programmability to be a key limiting factor to widespread adoption of CGRAs and FPGAs~\cite{fpgaMasses,DeSutter2013}.
The space of CGRA programmability is currently fragmented with incompatible, architecture-specific programming models.
The state of the art in programming FPGAs involves using a combination of vendor-supplied IP blocks,
hand-tuned hardware modules written using either low-level RTL or high-level synthesis tools,
and architecture-specific glue logic to communicate with off-chip components such as DRAM.
Hardware description languages (HDLs) like Verilog and VHDL are designed for explicit specification of hardware,
placing the burden on the user to solve the complexities of implementing their algorithm in hardware.

\section{High Level Synthesis}
High-level synthesis (HLS) tools like SDAccel~\cite{sdaccel}, Vivado HLS~\cite{vivadohls},
and Intel's OpenCL SDK~\cite{opencl_sdk} raise the level of abstraction compared to HDLs significantly.
For example, HLS tools allow programmers to write accelerator designs in terms of untimed, nested loops
and offer library functions for common operations like data transfer between a CPU host and the FPGA.
However, existing commercial HLS tools have all been built on top of software languages like C, OpenCL, and Matlab.
These software languages were originally built to target instruction-based processors like CPUs and GPUs.
This is clearly apparent in the language's semantics and memory model.
For example, the concept of pointers in C and C++ match a memory system which is flat, one dimensional, and uniformly accessible.
Describing the nuances of accessing memory in a more complicated system, such as the combination of main memory and
scratchpads on a GPU or FPGA inherently requires additional information in the language.
In practice, this is supported in HLS languages through optional user pragmas.
Consequently, although existing HLS tools raise the level of abstraction for targeting reconfigurable architectures,
they do so with an ad-hoc, often underspecified mix of software and hardware abstractions.
This, in turn, results in difficulties in HLS compiler implementations.
For instance, while SDAccel can convert nested loops into hardware state machines,
the language's compiler does not allow pipelining of loops at arbitrary nesting levels~\cite{vivado_userguide}.
Programmers must keep in mind that, despite the software programming abstractions, they must employ hardware, not software, optimization techniques when using HLS tools.
This makes it challenging to write code which produces fully optimized designs~\cite{nane2016survey}.


\section{Domain-Specific Languages}
As demands for specialization in hardware have been growing, so too have the demands for
specialization in software programming models for improved CPU and GPU performance and programmer productivity.
Domain-specific languages (DSLs) help to serve this goal by presenting the user with a very high level of abstraction
at the cost of a reduced number of possible operations and data structure types.
This focus in turn allows compilers to have more domain-specific semantic information
for each operation, enabling optimizations that would otherwise not be possible.
The use of DSLs has become particularly prominent lately in the field of machine learning (ML),
where new models and ideas are being tried ever more rapidly. Pytorch~\cite{pytorch} and TensorFlow~\cite{tensorflow},
for example, give a high level abstraction for building machine learning models for inference and training.

The benefit of domain-specific languages lie in their
high level of abstraction, which is typically almost entirely removed from any target
architecture. This level of abstraction makes DSLs a very promising choice for targeting reconfigurable
hardware.
It also gives the DSL compiler a great deal of freedom in what it can do to implement the
desired operations. Unfortunately, this degree of freedom also implies huge design spaces
which can be challenging to navigate automatically.
In existing DSL compilers that target reconfigurable hardware (usually FPGAs),
these complexities are solved using a kernel-based approach.
The DSL compiler performs domain-specific operations, then lowers the resulting
abstraction directly to a hardware implementation using either pre-existing, hand-written kernels in
an HDL~\cite{TODO} or implementations in a high level synthesis language~\cite{george14fpl}.
In both kernel-based techniques, the compilation path tends to miss key cross-operation optimizations
like fusion and tiling, resulting in excessive transfers to and from main memory.
In the latter case, the resulting generated accelerator designs suffer from the
same problems as hand-written HLS code.

\section{Contributions}
In this work, we propose a practical framework for automatic generation of
efficient FPGA accelerators starting from domain-specific languages.
The high level architecture of this framework is shown in Figure~\ref{fig:system-diag}.

We first present DSL compilers with a common intermediate abstraction composed of
parallel patterns like \emph{Map}, \emph{Reduce}, and \emph{FlatMap}.
In previous work, parallel patterns have shown to serve the dual purpose of raising the
level of abstraction for the
programmer~\cite{ecoop13sujeeth,pldi13halide}, and providing richer
semantic information to the compiler~\cite{delite-tecs14} about data parallelism
and access pattern information. We use them here because they have also been shown
to provide a common set of patterns across many computation domains, including
image processing, database processing, and machine learning~\cite{pldi13halide,ecoop13sujeeth}.
After domain-specific optimizations, a DSL compiler targeting
our framework lowers its operations to an implementation using these patterns.

Compilation in the framework is composed of two phases. In the first phase,
a high level compiler performs optimizations like loop fusion and loop and
data tiling on the parallel pattern IR (Figure~\ref{fig:system-diag}a.).
The IR is then automatically lowered into a hardware-specific intermediate
abstraction that explicitly captures information on parallelism,
locality, and memory access pattern at all levels of loop nesting using
parameterizable architectural templates (Figure~\ref{fig:system-diag}b.).
A second, hardware-specific compiler then takes the portion of the program
targeted for hardware acceleration and performs hardware-specific transformations and
optimizations, including specialization of on-chip memories based on access patterns (Figure~\ref{fig:system-diag}c.).
This phase also includes an optional automated search of the hardware design space
to improve performance (Figure~\ref{fig:system-diag}d.).

In this work, we first summarize high-level language abstractions required to create a new high-level synthesis language from the ground up, including syntax for managing memory, control, and accelerator-host interfaces on a reconfigurable architecture.
We suggest that this ``clean slate'' approach to high-level synthesis language design leads to a language which is semantically cleaner when targeting reconfigurable architectures, particularly when optimizing for data locality and parallelism.
These abstractions help programmer productivity and allow both the user and compiler to more easily optimize designs for improved performance.


We then describe a new domain specific language (DSL) and compiler framework called Spatial which implements these abstractions to support higher level, performance oriented hardware accelerator design.
Figure~\ref{fig:matmult} shows an example of a basic implementation of matrix multiplication in Spatial.
As this figure shows, Spatial code is like existing HLS languages in that programs are untimed and the language encourages accelerator designs to be expressed in terms of nested loops. However, unlike existing HLS tools, Spatial gives users more explicit control over the memory hierarchy through a library of on-chip and off-chip memory templates (e.g. the \texttt{DRAM} and \texttt{SRAM} in Figure~\ref{fig:matmult}).
Spatial automatically pipelines arbitrarily nested loops, and banks, buffers, and duplicates memories for the user based on parallel access patterns by default.
This is in contrast to modern HLS tools, which largely rely on the user to add explicit pragmas to their code in order make these optimizations.
Spatial also supports tuning of parameterized designs via automated design space exploration (DSE).
Unlike prior approaches~\cite{dhdl} which use variance-prone heuristic random search, Spatial employs an active machine learning framework called HyperMapper \cite{Bodin2016:PACT16} to drive exploration.
This tuning allows a single accelerator design to be quickly ported across target architectures and vendors with ease.

When targeting FPGAs, Spatial generates optimized, synthesizable Chisel code along with C++ code which can be used on a host CPU to administrate initialization and execution of the accelerator on the target FPGA.
Spatial currently supports Xilinx Ultrascale+ VU9P FPGAs on Amazon's EC2 F1 Instances, Xilinx Zynq-7000 and Ultrascale+ ZCU102 SoCs, and Altera DE1 and Arria 10 SoCs.
The constructs in Spatial are general across reconfigurable architectures, meaning Spatial programs can also be used to target CGRAs. In this paper, we demonstrate this by targeting our recently proposed Plasticine CGRA~\cite{plasticine}.




%Similarly, we introduce Spatial, a complete Domain Specific Language (DSL)
%that both provides the high-level abstractions needed to simplify programming spatial architectures and
%the hardware-specific optimizations over the entire application that are otherwise extremely challenging
%to do by hand in a low level language.

The contributions of this work are as follows:
\vspace{-5pt}
\begin{itemize}
  \item We discuss the abstractions required to describe target-agnostic accelerator designs for reconfigurable architectures (Section~\ref{background}). We then describe Spatial's implementation of these constructs (Section~\ref{language}) and the optimizations that these abstraction enables in the Spatial compiler (Section~\ref{compiler}).

  \vspace{5pt}

  \item We describe an improved method of fast, automated design parameter space exploration using HyperMapper (Section~\ref{dse}). This approach is evaluated in Section~\ref{evaluation}.

  \vspace{5pt}

  \item We evaluate Spatial's ability to efficiently express a wide variety of applications and
    target multiple architectures from the same source code. We demonstrate Spatial targeting two FPGAs and the Plasticine CGRA.
    We quantitatively compare Spatial to SDAccel on the VU9P FPGA on a diverse set of benchmarks (Section~\ref{evaluation}), showing a geometric mean speedup of $2.9\times$ with 42\% less code.
    We provide a qualitative comparison of Spatial to other related work in Section~\ref{related}.
\end{itemize}


\section{Outline}
Chapter~\ref{background} provides background on domain-specific languages and parallel patterns.
We also formalize the set of parallel patterns we will discuss in this work.
In Chapter~\ref{transformations} we discuss the high level transformations required to lower
parallel patterns into a form which can be more easily compiled to reconfigurable architectures.
Chapter~\ref{compiler} then discusses our specifications for an intermediate hardware abstraction.
We specify the implementation of this abstraction, the language Spatial, and describe the lowering
process from parallel patterns to Spatial.
We then describe Spatial's compiler and the transformations and hardware-specific optimizations that it performs.
Chapter~\ref{argon} describes the Argon compiler framework which both the high level and low level compiler are
built on, and highlights the primary improvements over its predecessor and lessons learned during its development.
In Chapter~\ref{related}, we discuss related work across the system, including a discussion of
prior work on automated tiling and high level synthesis languages.
Finally, we conclude in Chapter~\ref{conclusion} with a summary and discussion of future opportunities for optimization
and generalization in targeting FPGAs from DSLs using the compiler stack approach.
