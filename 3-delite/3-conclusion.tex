\section{Conclusion}

In this chapter, we introduced a series of compilation steps necessary to prepare
an intermediate representation composed of nested parallel patterns
for hardware compilation. We described a set of simple transformation
rules which can be used to automatically tile parallel patterns which exploit semantic information inherent within
these patterns and which place fewer restrictions on the programâ€™s memory accesses than previous work. We then presented a simple analysis step for annotating pipelining information for
consumption by the lower level compiler. Finally, we presented experimental results
for a set of benchmarks in the machine learning and data
querying domains which show that these compilation steps
provide performance improvements of up to $39.4\times$ with a
minimal impact on FPGA resource usage.
