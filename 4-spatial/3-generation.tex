\section{Hardware Generation}
\label{hardware}

In this section, we describe how the tiled intermediate representation is
translated into an efficient FPGA design. FPGAs are composed of various logic,
register, and memory resources. These resources are typically configured for
a specific hardware design using a hardware description language (HDL)
that is translated into an FPGA configuration file.

To separate concerns of this chapter from the subsequent chapter on Spatial,
and to define a ground set of templates required for Spatial's design,
we describe our approach to FPGA hardware generation here as generation rules
from PPL into a simple set of hardware templates. We describe how
these templates can be further refined
into a full hardware-specific intermediate representation in Chapter~\ref{spatial}.

\input{3-delite/figs/hwtemplates}

Table~\ref{t-hwtemplates} lists the templates and their corresponding PPL IR
constructs in three classes: memories, pipelined execution units,
and state machine controllers. \emph{Buffer}, \emph{Double buffer},
and \emph{Cache} are different on-chip memory templates intended to capture both
regular and data-dependent access patterns. In particular, the \emph{double buffer} template
is used to decouple execution stages and support dynamic rate mismatch between producer and consumer stages. Templates labeled as
\emph{Pipelined Execution Units} are used to support different kinds of innermost parallel patterns, as described in Table~\ref{t-hwtemplates}.
The \emph{Controller} templates implement a specific form of control flow using asynchronous handshaking signals. The \emph{Sequential},
\emph{Parallel}, and \emph{Metapipeline} controllers all orchestrate execution of a list of templates; \emph{Sequential} enforces
linear execution order, \emph{Parallel} enforces parallel execution with a barrier at the end, and \emph{MetaPipeline} enforces
pipelined execution. \emph{Tile Memory} controllers correspond to off-chip memory channels that load tiles of data into
one of the on-chip memory templates.
%We summarize the parallel pattern IR construct whose behavior each template captures in Table~\ref{t-hwtemplates}.
Each template can be composed with other templates.
For example, a \emph{Metapipeline controller} could be composed of multiple \emph{Parallel controller}s, each of which could
contain pipelined \emph{Vector} or \emph{Tree reduction} units.
%Next we discuss each of the templates in more detail.
We next describe the key features in the IR which we use to infer each of these template classes.



%\subsection{Memory Allocation}
\subsection{Memory Allocation}
Generating efficient FPGA hardware requires effective usage of on-chip memories (buffers).
Prior to generating MaxJ, we run an analysis pass to allocate buffers for arrays based on data access patterns and size.
%Since the maximum size of data inputs can not usually be inferred statically, we support the use of programmer hints about tile sizes to aid this analysis.
%Information from these annotations is propagated through the IR during this analysis.
All arrays with statically known sizes, such as array \emph{copies} generated in the tiling transformation described in
Section~\ref{transformations}, are assigned to buffers. Dynamically sized arrays are kept in main memory and we generate
caches for any non-affine accesses to these arrays.
We also track each memory's readers and writers and use this information
to instantiate a template with the appropriate word width and number of ports.

% Special memories are used for buffers which have unique access patterns. Streaming or ``one-touch'' data is
% either streamed directly to and from off-chip memory or realized using FIFOs on-chip depending on their usage.
% The tiling transformations described in Section
% represent data tile copies with explicit nodes in the IR. This greatly simplifies memory allocation analysis.
% to first determine where data
% is stored and how it is accessed.
% The goal of this analysis is to assign buffers
% to either on-chip or off-chip memories and select a memory template based on the properties
% of the buffer.
%In general, we use the following heuristics to allocate memories:
%\begin{itemize}
%  \item Buffers which have statically unknown dimensions (typically inputs and outputs)
%    are allocated off-chip. However, we allow programmers to provide hints to the compiler using
%    annotations which can enable more aggressive compiler optimizations. For example, the programmer
%    can hint that a certain buffer is ``cacheable'' -- meaning that the size of the buffer is typically
%    small enough (a few Kilobytes) to completely fit on-chip. Our analysis makes use of this information
%    and performs more aggressive on-chip memory allocation.
%  \item Buffers corresponding to tiles are allocated on-chip.
%  \item
%  \item Intermediate buffers in a \emph{metapipeline} (described in the next section) are realized
%    using \emph{Double Buffers}.
%\end{itemize}

%While the memory allocation analysis performs an initial assignment of templates to buffers, certain special
%buffers need to be \emph{promoted} to use the double buffer template. This is determined during
%metapipeline analysis, described in the next section.


%\subsection{Pipeline Execution Units}
\subsection{Pipeline Execution Units}
We generate parallelized and pipelined hardware when parallel patterns compute with scalar values,
as occurs for the innermost patterns.
We implemented templates for each
pipelined execution unit in Table~\ref{t-hwtemplates} using MaxJ language
constructs, and instantiate each template with the proper parameters (e.g., data type,
vector length) associated with the parallel pattern.  The MaxJ compiler
applies low-level hardware optimizations such as vectorization, code
scheduling, and fine-grained pipelining, and generates efficient hardware.  For
example, we instantiate a reduction tree for a MultiFold over an
array of scalar values, which is automatically pipelined by the MaxJ compiler.



%\subsection{Metapipelining}
